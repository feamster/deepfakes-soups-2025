\section{Call For Actions}
\label{sec:cfa}
Through a review of the current governance framework on AI-generated content (Section \ref{sec:history}) and discussion on why such a framework should be extended (Section \ref{sec:discuss}), we claim that as generative AI and AI-generated content are becoming part of the foundational infrastructure of the online information ecosystem, governance cannot stop at mitigating explicit harm. 

Building on our position and discussion, we outline recommendations for online platforms, AI developers and providers, regulators, and researchers to address challenges on how the surge of AI-generated content reshapes user experience, creator rights, and community value that ultimately influence online trust and safety. Rather than replacing existing measures, our recommendations aim to extend them towards a more proactive, ecosystem-oriented governance of AI-generated content.

\subsection{Recommendations for Online Platforms}
\paragraph{Incentivize Creators to Responsibly Use Generative AI in Content Creation.} As many new challenges arise from everyday uses of generative AI rather than bad actors, we suggest that online platforms turn their stance from primarily restricting and punishing ``bad'' AI-generated content to actively encouraging creators' responsible practices. For example, completely banning creators from producing AI slop or downgrading low-effort AI-generated content could be infeasible in cost and compromise legitimate use of AI in creativity (Section \ref{subsubsec:quality}). Instead, platforms could design incentives for high-quality AI-generated content involving meaningful human effort, creative insights, and intellectual contribution. Similarly, platforms could encourage users to disclose their use of generative AI regarding content authenticity and ownership. In particular, platforms could prioritize high-quality, well-disclosed AI-generated content in algorithmic recommendations and promotions, or offer additional rewards and program eligibility to creators who continue to comply with responsible practices in AI use.

Moreover, it is also important for platforms to inform creators about responsible practices for creating and sharing AI-generated content in order to avoid unintentional harms in routine uses, such as creating potentially realistic but potentially misleading content with a creative or entertainment purpose. A recent study on short video creators reveals their urgent needs for guidelines on using AI to create ``ethical and responsible'' content, particularly to avoid unintentional copyright violation and to clarify their responsibilities toward audiences as creators~\cite{kim2024unlocking}. However, studies reveal that to date, only a few platforms have offered concrete guidelines on how generative AI should be mindfully and responsibly used in content creation, such as AI-generated content quality guidelines~\cite{gao2026governance}. Therefore, we suggest that platforms develop responsible AI use guidelines as a part of the overall creator policies. In the guidelines, platforms could outline potential risks and harms at different stages of AI use in content creation with examples, and articulate best practices in creation and sharing, such as which kinds of AI use in content creation should be disclosed to avoid misleading the audience.


\paragraph{Empower Users through Educational Materials and Control Mechanisms} As AI-generated content in online spaces could cause invisible damage to user cognition, it is urgent to equip users with greater awareness and autonomy in navigating the flood of AI-generated content to counter it (Section \ref{subsubsec:invisibledamage}). However, as we reviewed in Section \ref{subsubsec:deepfakegovresponse} and Section \ref{subsubsec:AIGCgovresponse}, prior research developing user empowerment centers on promoting literacy of high-stakes deepfake manipulation. Moreover, such user support is also limited in the wild. Prior work reveals that in online platforms, labeling systems are usually the only tool for users to navigate AI-generated content, while other materials and tools for users to recognize such content and mitigate risks are rarely found~\cite{gao2026governance}. Yet, AI-generated content labels could themselves deepen confusion and undermine user trust, especially when mislabeling occurs (Section \ref{subsubsec:labelingtradeoff}).

Therefore, we suggest that platforms develop toolkits to proactively empower users when interacting with AI-generated content in the community. First, platforms could incorporate AI-generated content into their media literacy resources, which should provide concrete descriptions and examples of what generative AI and AI-generated content are, potential risks and harms around them, and how to navigate them in the community. Second, platforms could provide more controls over AI-generated content in user feeds through personalized moderation~\cite{jhaver2023personalizing} and recommendation controls~\cite{li2025beyond}, mechanisms that many platforms already offer on controlling content topics. We recommend that platforms tailor these existing mechanisms to AI-generated content, such as allowing users to choose whether to see AI-generated content and to what degree they want to be exposed to it.

%As these feed customization mechanisms usually work on content topics (e.g., violence) rather than provenance or production methods,

\subsection{Recommendations for AI Developers and Service Providers}
\paragraph{Extend Safety Guardrail and Watermark-based Provenance to Authorship Information.}
Current governance efforts around generative AI services largely focus on two technical levers: safety guardrails that prevent or filter harmful outputs, and provenance or watermarking schemes that help downstream actors detect whether content has been generated by a model. While these mechanisms are crucial for mitigating explicit harms, they provide little information about authorship, effort, or the degree of human involvement in AI-mediated creation. As a result, key questions around ownership and quality remain highly contested (Section~\ref{subsubsec:ownership}), and online platforms have limited signals for distinguishing between fully automated outputs and works where creators have invested substantial labor and expertise.

We therefore suggest that AI developers and providers consider extending existing provenance channels to include coarse-grained indicators of authorship and effort. For example, models and tooling could expose metadata such as prompt length and complexity for fully AI-generated materials, approximate generation time, the number of regeneration or variation steps requested, or simple measures of post-editing within the product interface


\subsection{Recommendations for Regulators}
\paragraph{Updating the current regulations for AI-generated content.} We regard several regulations as requiring updates to address emerging challenges arising from the prevalence of AI-generated content. First, although ownership of AI-generated content is inherently ambiguous, many generative AI services and online platforms still rely on existing copyright law to address ownership disputes (Section \ref{subsubsec:ownership}). Therefore, we see a need to update existing law to (re)define copyright for AI-generated content, especially given how AI models participate in copyright---a new challenge compared to earlier tensions among real human entities~\cite{samuelson2023generative}. Specifically, we recommend that regulators should not only define who ultimately owns the intellectual property of AI-generated content, but also recognize and allocate contributors among different parties: creators who create AI-generated content, the AI model and company, and the training data provider, across different uses of AI in content creation. Such reformulation could draw on current research on technical solutions and public perspectives on AI-generated content copyright (e.g.,~\cite{10.1145/3715336.3735683, lima2025public}). We envision this update could not only offer a clear legal framework on ownership disputes, but also help the platform better decide monetization and promotion as a reference.

Moreover, transparency in the distribution of AI-generated content also needs regulatory attention. As discussed in Section \ref{subsubsec:labelingtradeoff}, online platforms usually fulfill transparency around AI-generated content by requiring disclosure or auto-labeling content as AI-generated or not, where trade-offs could arise from the non-binary nature of AI-generated content and mislabels. Existing regulations on generative AI tend to focus on transparency of service providers, such as requiring the disclosure of the service as AI and tagging provenance signals on generated content. Few rules imposing responsibility on platforms for the transparency of AI-generated content they host (Section \ref{subsubsec:AIGCgovresponse}). Under this context, we suggest regulators consider incorporating transparency requirements around AI-generated content on online platforms into the general platform safety and transparency legal frameworks, such as those in the EU Digital Service Act. Rather than only mandating platforms to detect and label AI-generated content, such requirements could also ask platforms to provide appeal pipelines for AI-generated content labels, as is required by user rights for typical content moderation in the EU Digital Service Act, and report data on how AI-generated content is detected, labeled, and taken down.


\paragraph{Building guidelines for best practice on platform governance of AI-generated content}
As we have discussed, governance of AI-generated content in online spaces needs to be updated in light of emerging challenges that . Yet many online platforms continue to rely on existing content moderation frameworks. Although current governmental guidelines often address general AI harms, generative AI safety, and deepfake manipulation, few provide concrete guidance on the broader trust and safety implications of the online distribution of AI-generated content~\cite{luna2024navigating}. Yet, many online platforms still follow the existing content moderation framework. Although current governmental guidelines usually cover general AI harms, generative AI safety, and deepfake manipulation, limited guidelines have targeted the trust and safety of online distribution of AI-generated content in general~\cite {luna2024navigating}. Therefore, there is an urgent need of official guidance, including case examples on emerging harms, what harms are high-stakes and what are low-stakes, governance framework on minimum governance efforts for both online platforms and AI service providers. Such guidance could especially focus on emerging concerns like AI slop and suggestions on how to maintain community value through govern AI-generated content.

\subsection{Recommendations for Researchers}
1. Study more on new challenges and their effect on online trust and safety in the real world? like ownership, failures of labels, AI slop how users, especially the vulnerable population, react to new challenges?

measurement on what is happening around trust and safety (consequences)
in-situ

2. Develop countermeasure on new challenges and test their effectiveness?
such as contested ownership